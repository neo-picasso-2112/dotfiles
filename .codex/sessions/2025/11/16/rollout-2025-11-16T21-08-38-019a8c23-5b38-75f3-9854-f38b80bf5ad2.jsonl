{"timestamp":"2025-11-16T10:08:38.747Z","type":"session_meta","payload":{"id":"019a8c23-5b38-75f3-9854-f38b80bf5ad2","timestamp":"2025-11-16T10:08:38.712Z","cwd":"/Users/williamnguyen/repos","originator":"codex_cli_rs","cli_version":"0.58.0","instructions":"- **Git workflow**: Never push directly to `main` or `dev` branches. Always create a feature branch, commit changes there, push the branch, and create a merge/pull request for review.\n\n## Data Platform Architecture\n\n### Platform Overview\nJemena's data platform follows a **5-layer architecture** with strict dependency ordering. Each repository manages a distinct layer, and **state file outputs** from upstream repos are consumed by downstream repos via Terraform remote state.\n\n### Network Architecture WHY\n\n**Shared VPC Model** (owned by core-network account 234268347951):\n- **WHY Shared**: One VPC serves multiple application accounts. Reduces TGW attachment costs, simplifies routing, centralizes IP management.\n- **Prod VPC**: 10.32.0.0/16 (core-network-shared-prod-vpc)\n- **Nonprod VPC**: 10.34.0.0/16 (core-network-shared-nonprod-vpc)\n- **DMZ VPC**: 10.36.0.0/16 (core-network-shared-dmz-vpc)\n\n**Centralized Inspection**:\n- **WHY**: All inter-VPC traffic MUST traverse Palo Alto firewalls in core-security account (484357440923).\n- **Pattern**: Workload → Transit Gateway → core-security-fw-vpc → Palo Alto → destination\n- **Impact**: Cross-BU communication requires explicit firewall rules.\n\n**Centralized Endpoints**:\n- **WHY**: PrivateLink endpoints (RDS, KMS, S3, SSM) centralized in core-network account prevent per-account duplication.\n- **Pattern**: Separate endpoints for prod/nonprod to avoid tromboning through security VPC.\n\n**Subnet Allocation for Databricks**:\n- **app-datahub-prod**: 10.32.78.0/27 - 10.32.103.0/24 (3 subnets per workspace across 3 AZs)\n- **app-datahub-nonprod**: Equivalent range in 10.34.0.0/16\n- **WHY 3 subnets per workspace**: ENI subnet (/27), DB subnet (/27), Private/workload subnet (/24)\n\n**Critical Constraint**: Application accounts (app-datahub-prod, app-datahub-nonprod) do NOT own VPCs. Subnets are shared via AWS RAM. Networking changes require core-network team approval.\n\n### Repository Layers & Purpose\n\n#### Layer 1: AWS Cloud Resources (Foundation)\n- **`app-datahub-prod-databricks-aws-infra/`** - Prod AWS account (339712836516) resources\n- **`nonprod/app-datahub-nonprod-databricks-aws-infra/`** - Nonprod AWS account (851725449831) resources\n- **`core-network-databricks-vpc-components/`** - Shared VPC endpoints, security groups in core-network account\n\n**Purpose**: Provision AWS infrastructure (S3 buckets, IAM roles, KMS keys, security groups) + VPC networking components.\n\n**Critical Pattern**: Each BU has lab (dev/qa) and field (prod) workspace + catalog resources with medallion architecture (landing, bronze, silver, gold).\n\n#### Layer 2: Databricks Account Resources (Unity Catalog)\n- **`databricks-unity-catalog/`** - Account-level Databricks resources\n\n**Purpose**: Provision Databricks account-level infrastructure (metastore, catalogs, storage credentials, RBAC groups).\n\n**Dependencies**: Consumes AWS infra state for S3 buckets, IAM roles, VPC endpoints.\n\n**Unity Catalog Design Philosophy**:\n- **WHY Centralized Metastore**: Single source of truth for all workspaces across all BUs. Enables cross-BU data sharing and centralized audit logs.\n- **WHY Stage Distinction at Catalog Level** (NOT metastore): Allows granular lab→field promotion without metastore-level complexity.\n- **WHY Catalog-per-BU-per-Environment**: Blast radius isolation (changes to digital_lab don't impact corporate_field). Cost attribution via S3 bucket tagging.\n- **WHY Shared Metastore S3 Backend**: Renaming `app-datahub-{env}-s3-acc-metastore` bucket breaks ALL workspace bindings across ALL BUs.\n\n**Stacks**: Organized by BU (`digital-bu/`, `corporate-bu/`, `elec-network-bu/`) with prod/nonprod separation.\n\n#### Layer 3: Databricks Workspace Resources\n- **`databricks-workspaces/`** - Workspace-level Databricks resources\n\n**Purpose**: Configure workspace-specific resources (schemas, clusters, grants, SQL warehouses, instance profiles).\n\n**Dependencies**: Consumes state from AWS infra (S3, IAM, KMS) + Unity Catalog (catalog IDs, metastore, storage credentials).\n\n**Critical Patterns**:\n- **Medallion schemas**: landing → bronze → silver → gold with volumes and external locations\n- **Init scripts MANDATORY**: Clusters require CA certs, DNS config (BEPL), in-memory metastore. Without these, clusters fail to connect to internal systems.\n- **Service Principal OAuth M2M**: CI/CD auth pattern (not password-based)\n- **Enhanced grants**: RBAC on schemas/tables based on persona groups (engineers, analysts, scientists, viewers)\n\n#### Layer 4: Data Products (Application Code)\n- **`databricks-app-code-digital-analytics/`** - Digital Analytics data product\n- **`databricks-app-code-nri/`** - Network Reliability & Intelligence data product\n\n**Purpose**: Implement data pipelines, dbt models, and Databricks jobs deployed via **Databricks Asset Bundles (DABs)**.\n\n**Dependencies**: Requires provisioned workspaces from Layer 3.\n\n**Deployment Pattern**:\n- **lab target**: Development mode (auto-prefixed with `[dev username]`, schedules disabled)\n- **field target**: Production mode (scheduled jobs, alerts, full governance)\n- Deployed via GitLab CI/CD using `databricks bundle deploy -t <target>`\n\n#### Layer 5: Governance Applications\n- **`datamesh-manager-prod/`** - Data Mesh Manager (DMM) governance platform\n- **`network-model-ewb-prod/`** - Energy Workbench network modeling application\n\n**Purpose**: Provide governance, metadata management, and domain-specific applications.\n\n**Infrastructure**: ECS Fargate services with ALB, RDS PostgreSQL, cross-account DNS via AssumeRole.\n\n### Deployment Order WHY\n\n**NEVER deploy out of order. Dependencies MUST be satisfied first.**\n\n```\n1. AWS Cloud Resources (app-datahub-*-databricks-aws-infra)\n       ↓ (outputs: S3 buckets, IAM roles, KMS keys, security groups)\n2. Unity Catalog (databricks-unity-catalog)\n       ↓ (outputs: metastore ID, catalog IDs, storage credentials)\n3. Workspaces (databricks-workspaces)\n       ↓ (outputs: workspace configs, cluster policies, schemas)\n4. Data Products (databricks-app-code-*)\n       ↓ (deployed as DABs to workspaces)\n5. Governance Apps (datamesh-manager-*, network-model-*)\n```\n\n**WHY Deployment Order is Immutable**:\n- **Metastore binding**: Once a workspace binds to a metastore (Layer 2), it CANNOT be rebound without destroying the workspace.\n- **IAM role ARNs**: Workspace creation requires cross-account role ARN from Layer 1. If role doesn't exist, workspace creation fails.\n- **Catalog storage**: External locations require S3 buckets + storage credentials. If bucket doesn't exist, catalog creation fails.\n- **Blast Radius**: Deploying Layer 3 before Layer 2 = workspaces fail to bind. Deploying Layer 2 before Layer 1 = \"S3 bucket not found\" errors.\n\n### State File Coupling WHY\n\n**GitLab HTTP Backend Pattern**:\n- All state stored in GitLab projects (identified by project IDs in `backend.tf`)\n- Output consumption via `terraform_remote_state` in `dependencies.tf` or `upstream_stacks.tf`\n- **WHY This Coupling**: DRY principle - resource names/ARNs defined once, consumed downstream. Prevents drift from hardcoded values.\n\n**Breaking Changes Cascade**:\n- Renaming S3 bucket in Layer 1 breaks Layer 2 (UC) and Layer 3 (workspaces)\n- Audit trail: GitLab MRs show exactly which downstream repos need updates\n- Recovery: If upstream state corrupted, downstream repos cannot plan/apply until state restored\n\n### Environment Patterns\n\n#### Nonprod AWS Account (851725449831)\n- **Purpose**: Platform testing and development **before** production deployment\n- **Workspaces**: digital-lab, corporate-lab, elec-network-lab, sparky-lab (dev/qa environments)\n- **Users**: Platform engineers only (no business users)\n- **Pattern**: Changes tested here first, then promoted to prod\n- **Unique Resources**: Athena cross-account queries, Sparky BU (testing only)\n\n#### Prod AWS Account (339712836516)\n- **Purpose**: Production workloads with business users\n- **Workspaces**: digital-field, corporate-field, elec-network-field (production environments)\n- **Users**: Data engineers, analysts, scientists actively building pipelines\n- **Pattern**: Changes deployed only after nonprod validation\n\n**Critical Insight**: When modifying platform infrastructure (AWS resources, Unity Catalog, workspaces), **ALWAYS deploy to nonprod first**, validate, then deploy to prod. Data product code (dbt, jobs) follows same pattern (lab → field).\n\n### Security Patterns\n\n#### Encryption\n- **KMS Keys**: Separate customer-managed keys per BU (digital, corporate, elec-network) + shared metastore key\n- **S3 Encryption**: All buckets encrypted with KMS (SSE-KMS)\n- **TLS**: ALBs enforce TLS 1.3 (`ELBSecurityPolicy-TLS13-1-2-2021-06`)\n\n#### IAM & Cross-Account Access\n- **External ID Validation**: All cross-account assume roles use Databricks account ID as external ID\n- **Least Privilege**: IAM policies scoped to specific resources (bucket ARNs, VPC constraints)\n- **Instance Profiles**: Workspace clusters attach profiles for S3/catalog access\n- **Service Principal Auth**: CI/CD uses SP OAuth tokens (stored in GitLab secrets)\n\n#### Secrets Management\n- **AWS Secrets Manager**: Database credentials, OAuth tokens, certificates\n- **GitLab Group Variables**: Service principal credentials (synced from Databricks)\n- **Parameter Store**: Application configs (`/service/network-model-ewb/*`)\n\n#### Network Security\n- **Security Groups**: Explicit egress rules for Databricks control plane, databases, APIs\n- **VPC Endpoints**: Private connectivity (REST, SCC relay, FEPL) for Databricks\n- **Cross-Account DNS**: Route53 private hosted zones managed via AssumeRole to core-network account\n\n### Key Architectural Insights\n\n1. **Core Network Dependency**: `core-network-databricks-vpc-components` is foundational for ALL private connectivity. Changes here cascade to all workspaces.\n\n2. **Data Layer Separation**: `network-model-datastores` separates data persistence from compute (allows independent scaling/upgrades).\n\n3. **Module Reusability**: Shared modules (`bucket`, `workspace-cloud-resources`, `catalog-cloud-resources`, `medallion-schemas`, `enhanced-grants`) enforce consistency.\n\n4. **Multi-BU Isolation**: Business units (Digital, Corporate, Elec-Network) have isolated workspaces and catalogs with selective cross-BU read-only bindings.\n\n5. **CI/CD Governance**: GitLab pipelines enforce terraform plan → governance checks (Checkov, OPA) → manual approval → apply.\n\n## Using MCP servers\n- Documentation memory bank lives under the `docs/` folder.\n  - Each repo mirrors its name under `docs/<repository-name>`; from the repo root, `cd docs/<repository-name>`. For example, `docs/app-datahub-prod-databricks-aws-infra`).\n  - If a mirror folder is missing, inspect `docs/` for the closest match or ask for clarification before proceeding.\n- Generate diagrams with the `aws-diagram` MCP tool.\n  - Supply the current repository as `workspace_dir`, store the output in `docs/generated-diagrams/<descriptive-name>.png`, and then use `open` on that path to review the image.\n- Choose Cost Explorer MCP when you need lightweight analytics or forecasts, want to minimize IAM blast radius, and expect to work only with Cost Explorer data (e.g., finance teams triaging monthly deltas, engineers checking one service’s spike).\n- Choose Billing & Cost Management MCP when you need an end-to-end FinOps cockpit—budgets, anomaly hunting, optimization recommendations, Free Tier tracking, savings-plans guidance, Storage Lens queries—or when you’d otherwise have to orchestrate several MCP servers for the same analysis pipeline.\n","source":"cli","model_provider":"openai"}}
